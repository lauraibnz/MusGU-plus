<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>MusGU+ Framework</title>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&amp;display=swap" rel="stylesheet"/>
<link href="styles.css" rel="stylesheet">
<style>
body {
  font-family: 'Open Sans', sans-serif;
  margin: 0 auto;
  padding: 20px;
  background: #fff;
  opacity: 0;
  transition: opacity 0.1s ease-in;
}

body.ready {
  opacity: 1;
}
.header {
  text-align: center;
  margin-bottom: 20px;
}
.header h1 {
  font-size: 2em;
  margin-bottom: 0.5em;
}
.header p {
  font-size: 1.1em;
  color: #666;
}
.controls {
  background: white;
  padding: 0;
  margin-bottom: 5px;
  display: flex;
  flex-direction: column;
  gap: 0;
  width: 100%;
  max-width: 1200px;
  margin-left: auto;
  margin-right: auto;
  box-sizing: border-box;
}

.active-filters-container {
  display: flex;
  flex-wrap: wrap;
  gap: 12px;
  padding: 6px 0;
  min-height: 40px;
  align-items: center;
  width: 100%;
  max-width: 1200px;
  box-sizing: border-box;
}

.active-filters-label {
  font-size: 14px;
  font-weight: bold;
  color: #000;
  white-space: nowrap;
  flex-shrink: 0; /* Prevent label from shrinking */
}

.active-filters-tags {
  display: flex;
  flex-wrap: wrap;
  gap: 6px;
  flex: 1 1 auto;
  min-width: 0; /* Critical: allows flex item to shrink below content size */
}

.active-filter-tag {
  padding: 3px 8px;
  background: #6c757d;
  color: white;
  border: 1px solid #6c757d;
  border-radius: 12px;
  font-size: 10px;
  cursor: pointer;
  transition: all 0.2s ease;
  display: inline-flex;
  align-items: center;
  gap: 4px;
  flex-shrink: 0;
  height: 28px;
  box-sizing: border-box;
}

.active-filter-tag:hover {
  background: #5a6268;
  border-color: #5a6268;
}

.active-filter-tag .remove-icon {
  font-weight: bold;
  font-size: 11px;
}

.clear-filters-inline {
  padding: 3px 8px;
  background: #6c757d;
  color: white;
  border: 1px solid #6c757d;
  border-radius: 12px;
  cursor: pointer;
  font-size: 10px;
  transition: background 0.2s ease, border-color 0.2s ease;
  white-space: nowrap;
  display: none;
  flex-shrink: 0; /* Don't allow button to shrink */
  height: 28px;
  box-sizing: border-box;
}

.clear-filters-inline.visible {
  display: inline-flex;
  align-items: center;
}

.clear-filters-inline:hover {
  background: #5a6268;
  border-color: #5a6268;
}

.search-container {
  display: none; /* Hide search for now, keep functionality */
  align-items: center;
  justify-content: flex-start;
  width: 100%;
}
.search-box-wrapper {
  display: flex;
  justify-content: flex-start;
}
.controls input, .controls select {
  padding: 8px 12px;
  margin: 5px;
  border: 1px solid #ddd;
  border-radius: 4px;
  font-size: 14px;
}
.search-box {
  width: 300px;
}

/* Applications tags section */
#applications-wrapper {
  width: 100%;
  max-width: 1200px;
  margin: 0 auto 20px auto;
  box-sizing: border-box;
}

.applications-section {
  margin: 0;
  text-align: center;
}

.applications-title {
  font-size: 16px;
  font-weight: 600;
  color: #000;
  margin: 0 0 15px 0;
}

.applications-tags-container {
  display: flex;
  flex-wrap: wrap;
  justify-content: center;
  align-items: center;
  gap: 8px;
  padding: 0 0 10px 0;
}

.application-tag {
  padding: 4px 10px;
  background: #f8f9fa;
  border: 1px solid #dee2e6;
  border-radius: 12px;
  font-size: 12px;
  cursor: pointer;
  transition: all 0.2s ease;
  user-select: none;
  font-weight: 500;
  white-space: nowrap;
}

.application-tag:hover {
  background: #e9ecef;
  border-color: #adb5bd;
}

.application-tag.active {
  background: #6c757d;
  color: white;
  border-color: #6c757d;
}

#musgu-table {
  width: auto;
  max-width: fit-content;
  border-collapse: separate;
  border-spacing: 2px 2px;
  font-size: 13px;
  background: white;
  box-shadow: none;
  margin: 0 auto;
  table-layout: auto;
}

#musgu-table.calculating {
  visibility: hidden;
}
#included-table {
  overflow-x: auto;
  width: 100%;
}
#musgu-table thead {
  background: white;
  color: #333;
  border-bottom: 1px solid #000;
}
#musgu-table th {
  padding: 12px 4px;
  text-align: center;
  font-weight: bold;
  border: none;
  border-bottom: 1px solid #000;
}
#musgu-table td {
  padding: 6px 4px; /* Increased from 6px to 8px for more vertical space */
  border: none;
  text-align: center;
}
.name-cell {
  text-align: left !important;
  font-weight: bold;
  border-right: 1px solid #000;
  width: auto;
  min-width: fit-content; /* Changed from max-content to fit-content */
  white-space: normal;
  max-width: none; /* Removed max-width constraint */
}
.model-name {
  font-weight: bold;
  line-height: 1.1; /* Increased from 1.1 */
  word-wrap: break-word;
  white-space: nowrap; /* Keep model names on one line */
}
.affiliation {
  font-size: 0.85em;
  color: #666;
  font-weight: normal;
  margin-top: 5px; /* Increased from 5px */
  line-height: 1.1; /* Increased from 1.1 */
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
  max-width: 100%;
  display: block; /* Ensure it's a block element */
  padding-bottom: 2px; /* Add padding to prevent descender clipping */
}

.org {
  text-align: left !important;
  font-size: 0.9em;
  color: #333;
  font-weight: bold;
  border-right: 1px solid #000;
  width: 0.1%;
  white-space: nowrap;
}
.score {
  font-weight: 600;
  background: #f0f0f0;
}
.data-cell {
  cursor: help;
}
.high {
  color: #2d6a4f;
  background: #c7f0d8;
}
.partial {
  color: #e67700;
  background: #ffd699;
}
.low {
  color: #c1121f;
  background: #ffb3ba;
}
.empty {
  color: #999;
  background: #f5f5f5;
}

.main-header th {
  font-size: 14px;
  padding: 10px;
  position: relative;
}

.main-header th:first-child,
.main-header th:nth-child(2) {
  width: 0.1%;
  white-space: nowrap;
}

.dimension-header-cell {
  display: flex;
  flex-direction: row;
  align-items: center;
  justify-content: center;
  gap: 8px;
}

.dimension-name {
  text-align: center;
}

.dimension-filter-tag {
  padding: 4px 10px;
  background: #f8f9fa;
  border: 1px solid #dee2e6;
  border-radius: 12px;
  font-size: 10px;
  cursor: pointer;
  transition: all 0.2s ease;
  user-select: none;
  font-weight: 500;
  white-space: nowrap;
}

.dimension-filter-tag:hover {
  background: #e9ecef;
  border-color: #adb5bd;
}

.dimension-filter-tag.active {
  background: #6c757d;
  color: white;
  border-color: #6c757d;
}

.second-header th {
  font-size: 12px;
  padding: 8px 4px;
  position: relative;
  width: 80px;
  vertical-align: top;
  white-space: normal;
  overflow: visible; /* Allow expanded tags to overflow */
}

.criterion-header-wrapper {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  gap: 6px;
  width: 100%;
  overflow: visible; /* Allow expanded tags to overflow */
}

.criterion-tags {
  display: inline-flex;
  flex-wrap: wrap;
  gap: 4px;
  justify-content: center;
  align-items: center;
  max-width: 150px;
  overflow: visible; /* Allow content to be visible */
}

/* When there's only 1 visible tag (no second tag, no expand button), make it narrower */
.criterion-tags:has(.visible-tag:only-of-type) {
  max-width: fit-content; /* Only as wide as the single tag */
}

/* When there are exactly 2 visible tags (no expand button), force them to wrap */
.criterion-tags:has(.visible-tag:nth-of-type(2)):not(:has(.expand-tags-btn)) {
  max-width: 60px; /* Force narrower width to wrap 2 tags */
}

.tag-group {
  display: inline-flex;
  gap: 4px;
  flex-wrap: nowrap; /* Keep tag and button together */
  align-items: center;
  flex-shrink: 0; /* Prevent the group from shrinking */
}

.criterion-tags.expanded {
  position: relative;
  max-width: none !important;
  flex-wrap: wrap !important;
  min-height: 60px; /* Reserve space for expanded tags */
}

.criterion-tags.expanded .tag-group {
  flex-wrap: wrap;
}

.hidden-tag {
  display: none !important;
}

.criterion-tags.expanded .hidden-tag {
  display: inline-flex !important;
  order: 999; /* Push hidden tags to the end when expanded */
}

/* Force first hidden tag to start on new line after button */
.criterion-tags.expanded .hidden-tag:first-of-type {
  margin-left: 100%; /* Force break before first hidden tag */
}

.expand-tags-btn {
  padding: 2px 1px;
  background: #e9ecef;
  border: 1px solid #dee2e6;
  border-radius: 10px;
  font-size: 8.5px;
  cursor: pointer;
  transition: all 0.2s ease;
  user-select: none;
  white-space: nowrap;
  color: #495057;
  font-weight: 500;
  display: inline-flex;
  align-items: center;
  flex-shrink: 0;
  min-width: 12px;
  justify-content: center;
  order: 100; /* Keep button after visible tags but before hidden tags */
}

.expand-tags-btn:hover {
  background: #dee2e6;
  border-color: #adb5bd;
}

.criterion-tag {
  padding: 1px 4px;
  background: #f8f9fa;
  border: 1px solid #dee2e6;
  border-radius: 8px;
  cursor: pointer;
  font-size: 9px;
  white-space: nowrap;
  transition: all 0.2s ease;
  flex-shrink: 0;
  user-select: none;
  display: inline-flex;
  align-items: center;
}

.criterion-tag:hover {
  background: #e9ecef;
  border-color: #adb5bd;
}

.criterion-tag.active {
  background: #6c757d;
  color: white;
  border-color: #6c757d;
}

.expand-tags-btn {
  padding: 2px 6px;
  background: #e9ecef;
  border: 1px solid #dee2e6;
  border-radius: 8px;
  font-size: 9px;
  cursor: pointer;
  transition: all 0.2s ease;
  user-select: none;
  color: #495057;
  flex-shrink: 0;
}

.criterion-header-wrapper {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  gap: 4px;
}

.filter-icon {
  cursor: pointer;
  font-size: 11px;
  color: #6c757d;
  padding: 2px 4px;
  border-radius: 2px;
  transition: all 0.2s ease;
  line-height: 1;
  font-weight: normal;
  display: none; /* Hide the SVG filter icon for now */
  vertical-align: middle;
}

.filter-icon svg {
  display: block;
  width: 9px;
  height: 7px;
  fill: currentColor;
}

.filter-icon:hover {
  background: #e9ecef;
  color: #495057;
}

.filter-icon.has-active {
  color: #fff;
  background: #6c757d;
}

.filter-popup {
  display: none;
  position: absolute;
  top: 100%;
  left: 50%;
  transform: translateX(-50%);
  background: white;
  border: 2px solid #dee2e6;
  border-radius: 8px;
  padding: 12px;
  box-shadow: 0 4px 12px rgba(0,0,0,0.15);
  z-index: 1000;
  min-width: 200px;
  margin-top: 4px;
}

.filter-popup.active {
  display: block;
}

.filter-popup-title {
  font-size: 11px;
  font-weight: 600;
  color: #495057;
  margin-bottom: 8px;
  text-align: center;
}

.filter-popup-tags {
  display: flex;
  flex-wrap: wrap;
  gap: 6px;
  justify-content: center;
}

.filter-popup-tag {
  padding: 4px 10px;
  background: #f8f9fa;
  border: 1px solid #dee2e6;
  border-radius: 12px;
  font-size: 11px;
  cursor: pointer;
  transition: all 0.2s ease;
  user-select: none;
}

.filter-popup-tag:hover {
  background: #e9ecef;
  border-color: #adb5bd;
}

.filter-popup-tag.active {
  background: #6c757d;
  color: white;
  border-color: #6c757d;
}

#footer {
  margin-top: 30px;
  padding: 30px 40px;
  margin-bottom: 0;
  background: #f8f8f8;
  border-top: 3px solid #e0e0e0;
  text-align: center;
  font-size: 14px;
  color: #666;
}
.legend {
  display: flex;
  justify-content: center;
  align-items: center;
  gap: 15px;
  margin: 15px auto;
  flex-wrap: wrap;
  width: auto;
  max-width: 1200px;
}
.legend-items {
  display: flex;
  justify-content: center;
  gap: 15px;
  flex-wrap: wrap;
  flex: 1;
}
.legend-item {
  display: flex;
  align-items: center;
  gap: 6px;
}
.legend-symbol {
  font-size: 16px;
  font-weight: bold;
}
.sortable {
  cursor: pointer;
  user-select: none;
}
.sortable:hover {
  background: #d8d8d8;
}
.sort-arrow {
  font-size: 0.7em;
  margin-left: 4px;
  opacity: 0.3;
  display: inline-block;
}
.sortable.active .sort-arrow {
  opacity: 1;
}
.clear-filters-btn {
  padding: 8px 16px;
  background: #6c757d;
  color: white;
  border: 1px solid #6c757d;
  border-radius: 20px;
  cursor: pointer;
  font-size: 12px;
  transition: background 0.2s ease, border-color 0.2s ease;
  white-space: nowrap;
}
.clear-filters-btn:hover {
  background: #5a6268;
  border-color: #5a6268;
}
#description {
  background-color: #f8f8f8;
  padding: 1.5em;
  margin: 5px auto 5px;
  max-width: 1200px;
  font-size: 1em;
  line-height: 1.6;
  border-radius: 4px;
}
#description ul {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}
#table-guide {
  background-color: #f8f8f8;
  padding: 1em 1.5em;
  margin: 0 auto 0;
  max-width: 1200px;
  font-size: 1em;
  line-height: 1.6;
  border-radius: 4px;
}
#table-guide p {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}
#table-guide ul {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}
</style>
</link></head>
<body>
<div class="header">
<h1>MusGU+: Musician-Centered Evaluation Framework</h1>
</div>
<div id="description">
<p style="margin-top: 0;">
    The <strong>Music-Generative Usable+ AI (MusGU+)</strong> framework is a musician-centered evaluation framework designed to assess how generative music models can be adapted, used, and controlled in real-world creative contexts. 
    The framework evaluates models along three complementary dimensions, with each dimension addressing a key question from the musician's perspective:
  </p>
<ul>
<li><strong>Adaptability</strong> â€” <em>Can I realistically adapt this model to my own data?</em></li>
<li><strong>Usability</strong> â€” <em>Can I access, run, and integrate this model into my music-making workflow?</em></li>
<li><strong>Controllability</strong> â€” <em>Can I guide the model in musically meaningful and interpretable ways?</em></li>
</ul>
<p style="margin-bottom: 0;">
    ðŸ“– Read the <a href="framework.html" style="color: #0066cc;">detailed evaluation criteria</a>
</p>
</div>
<h2 style="text-align: center; margin: 20px auto 5px; max-width: 1200px; font-size: 1.3em; margin-bottom: 15px;">A Discovery Tool for Generative Music AI</h2>
<div class="controls">
<div class="active-filters-container" id="active-filters">
<div class="active-filters-label">Active Filters:</div>
<div class="active-filters-tags">
<!-- Active criterion filter tags will appear here -->
</div>
<button class="clear-filters-inline" id="clear-filters-inline">âœ• Clear All</button>
</div>
<div class="search-container">
<div class="search-box-wrapper">
<input class="search search-box" placeholder="Search models..."/>
</div>
</div>
</div>
<div id="applications-wrapper">
<!-- Applications section will be injected here by Python script -->
<div class="applications-section">
<h3 class="applications-title">Musical Applications</h3>
<div class="applications-tags-container">
<span class="application-tag" data-application="MIDI-to-audio">MIDI-to-audio</span>
<span class="application-tag" data-application="audio synthesis">audio synthesis</span>
<span class="application-tag" data-application="continuation">continuation</span>
<span class="application-tag" data-application="editing">editing</span>
<span class="application-tag" data-application="full song generation">full song generation</span>
<span class="application-tag" data-application="lyrics-to-song">lyrics-to-song</span>
<span class="application-tag" data-application="remixing">remixing</span>
<span class="application-tag" data-application="style transfer">style transfer</span>
<span class="application-tag" data-application="text-to-music">text-to-music</span>
</div>
</div>
</div>
<div id="included-table">
<!-- Table will be injected here by Python script -->
<table id="musgu-table">
<thead>
<tr class="main-header"><th class="sortable" data-sort="name" data-type="text">Model <span class="sort-arrow">â–´â–¾</span></th><th class="sortable" colspan="5" data-sort="adaptability" data-type="number"><div class="dimension-header-cell"><span class="dimension-name">Adaptability <span class="sort-arrow">â–´â–¾</span></span><div class="dimension-filter-tag" data-filter="adaptability" data-threshold="60">â‰¥60%</div></div></th><th class="sortable" colspan="6" data-sort="usability" data-type="number"><div class="dimension-header-cell"><span class="dimension-name">Usability <span class="sort-arrow">â–´â–¾</span></span><div class="dimension-filter-tag" data-filter="usability" data-threshold="60">â‰¥60%</div></div></th><th class="sortable" colspan="4" data-sort="controllability" data-type="number"><div class="dimension-header-cell"><span class="dimension-name">Controllability <span class="sort-arrow">â–´â–¾</span></span><div class="dimension-filter-tag" data-filter="controllability" data-threshold="60">â‰¥60%</div></div></th></tr>
<tr class="second-header"><th></th><th><div class="criterion-header-wrapper"><span>Hardware<br/>Requirements</span><div class="criterion-tags" data-criterion="hardware"><span class="criterion-tag" data-criterion="hardware" data-tag="CPU">CPU</span><span class="expand-tags-btn" data-criterion="hardware">+0</span></div></div></th><th><div class="criterion-header-wrapper"><span>Dataset<br/>Size</span><div class="criterion-tags" data-criterion="dataset"><span class="criterion-tag" data-criterion="dataset" data-tag="small dataset">small dataset</span><span class="expand-tags-btn" data-criterion="dataset">+0</span></div></div></th><th><div class="criterion-header-wrapper"><span>Adaptation<br/>Pathways</span><div class="criterion-tags" data-criterion="adaptation"><span class="criterion-tag" data-criterion="adaptation" data-tag="LoRA">LoRA</span><span class="criterion-tag" data-criterion="adaptation" data-tag="training">training</span><span class="criterion-tag" data-criterion="adaptation" data-tag="fine-tuning">fine-tuning</span><span class="criterion-tag" data-criterion="adaptation" data-tag="prior training">prior training</span><span class="criterion-tag" data-criterion="adaptation" data-tag="pretrained codec">pretrained codec</span><span class="criterion-tag" data-criterion="adaptation" data-tag="pretrained checkpoints">pretrained checkpoints</span><span class="expand-tags-btn" data-criterion="adaptation">+0</span></div></div></th><th><div class="criterion-header-wrapper"><span>Technical<br/>Barriers</span><div class="criterion-tags" data-criterion="technical"><span class="criterion-tag" data-criterion="technical" data-tag="GUI">GUI</span><span class="criterion-tag" data-criterion="technical" data-tag="CLI">CLI</span><span class="criterion-tag" data-criterion="technical" data-tag="Colab">Colab</span><span class="criterion-tag" data-criterion="technical" data-tag="tutorial">tutorial</span><span class="criterion-tag" data-criterion="technical" data-tag="drag-and-drop">drag-and-drop</span><span class="expand-tags-btn" data-criterion="technical">+0</span></div></div></th><th><div class="criterion-header-wrapper"><span>Model<br/>Redistribution</span></div></th><th><div class="criterion-header-wrapper"><span>Interface<br/>Availability</span><div class="criterion-tags" data-criterion="interface"><span class="criterion-tag" data-criterion="interface" data-tag="AU">AU</span><span class="criterion-tag" data-criterion="interface" data-tag="VST">VST</span><span class="criterion-tag" data-criterion="interface" data-tag="Colab">Colab</span><span class="criterion-tag" data-criterion="interface" data-tag="web UI">web UI</span><span class="criterion-tag" data-criterion="interface" data-tag="Gradio">Gradio</span><span class="criterion-tag" data-criterion="interface" data-tag="iOS app">iOS app</span><span class="criterion-tag" data-criterion="interface" data-tag="Max/MSP">Max/MSP</span><span class="criterion-tag" data-criterion="interface" data-tag="Max4Live">Max4Live</span><span class="criterion-tag" data-criterion="interface" data-tag="PureData">PureData</span><span class="criterion-tag" data-criterion="interface" data-tag="Android app">Android app</span><span class="expand-tags-btn" data-criterion="interface">+0</span></div></div></th><th><div class="criterion-header-wrapper"><span>Access<br/>Restrictions</span></div></th><th><div class="criterion-header-wrapper"><span>Real-time<br/>Capabilities</span><div class="criterion-tags" data-criterion="realtime"><span class="criterion-tag" data-criterion="realtime" data-tag="real-time">real-time</span><span class="expand-tags-btn" data-criterion="realtime">+0</span></div></div></th><th><div class="criterion-header-wrapper"><span>Workflow<br/>Integration</span><div class="criterion-tags" data-criterion="workflow"><span class="criterion-tag" data-criterion="workflow" data-tag="DAW">DAW</span><span class="criterion-tag" data-criterion="workflow" data-tag="hardware">hardware</span><span class="criterion-tag" data-criterion="workflow" data-tag="visual programming">visual programming</span><span class="expand-tags-btn" data-criterion="workflow">+0</span></div></div></th><th><div class="criterion-header-wrapper"><span>Output<br/>Licensing</span></div></th><th><div class="criterion-header-wrapper"><span>Community<br/>Support</span><div class="criterion-tags" data-criterion="community"><span class="criterion-tag" data-criterion="community" data-tag="Forum">Forum</span><span class="criterion-tag" data-criterion="community" data-tag="Discord">Discord</span><span class="criterion-tag" data-criterion="community" data-tag="Help center">Help center</span><span class="criterion-tag" data-criterion="community" data-tag="GitHub Issues">GitHub Issues</span><span class="criterion-tag" data-criterion="community" data-tag="GitHub Discussions">GitHub Discussions</span><span class="expand-tags-btn" data-criterion="community">+0</span></div></div></th><th><div class="criterion-header-wrapper"><span>Conditioning<br/>Inputs</span><div class="criterion-tags" data-criterion="conditioning"><span class="criterion-tag" data-criterion="conditioning" data-tag="MIDI">MIDI</span><span class="criterion-tag" data-criterion="conditioning" data-tag="text">text</span><span class="criterion-tag" data-criterion="conditioning" data-tag="audio">audio</span><span class="criterion-tag" data-criterion="conditioning" data-tag="timing">timing</span><span class="criterion-tag" data-criterion="conditioning" data-tag="melody">melody</span><span class="criterion-tag" data-criterion="conditioning" data-tag="lyrics">lyrics</span><span class="criterion-tag" data-criterion="conditioning" data-tag="section">section</span><span class="criterion-tag" data-criterion="conditioning" data-tag="style tags">style tags</span><span class="expand-tags-btn" data-criterion="conditioning">+0</span></div></div></th><th><div class="criterion-header-wrapper"><span>Time-Varying<br/>Control</span></div></th><th><div class="criterion-header-wrapper"><span>Feature<br/>Disentanglement</span><div class="criterion-tags" data-criterion="disentanglement"><span class="criterion-tag" data-criterion="disentanglement" data-tag="pitch">pitch</span><span class="criterion-tag" data-criterion="disentanglement" data-tag="timbre">timbre</span><span class="criterion-tag" data-criterion="disentanglement" data-tag="loudness">loudness</span><span class="criterion-tag" data-criterion="disentanglement" data-tag="structure">structure</span><span class="expand-tags-btn" data-criterion="disentanglement">+0</span></div></div></th><th><div class="criterion-header-wrapper"><span>Control<br/>Parameters</span><div class="criterion-tags" data-criterion="parameters"><span class="criterion-tag" data-criterion="parameters" data-tag="duration">duration</span><span class="criterion-tag" data-criterion="parameters" data-tag="randomness">randomness</span><span class="criterion-tag" data-criterion="parameters" data-tag="variability">variability</span><span class="criterion-tag" data-criterion="parameters" data-tag="latent noise">latent noise</span><span class="criterion-tag" data-criterion="parameters" data-tag="latent prior">latent prior</span><span class="criterion-tag" data-criterion="parameters" data-tag="diffusion steps">diffusion steps</span><span class="criterion-tag" data-criterion="parameters" data-tag="sampling strategy">sampling strategy</span><span class="criterion-tag" data-criterion="parameters" data-tag="latent manipulation">latent manipulation</span><span class="criterion-tag" data-criterion="parameters" data-tag="conditioning strength">conditioning strength</span><span class="expand-tags-btn" data-criterion="parameters">+0</span></div></div></th></tr>
</thead>
<tbody>
<tr class="row-a" data-adaptability="70" data-affiliation="Google Magenta" data-applications="MIDI-to-audio,audio synthesis,style transfer" data-controllability="100" data-name="DDSP-VST" data-overall="90" data-tags="dataset:small dataset,adaptation:training,technical:Colab,interface:AU,interface:VST,realtime:real-time,workflow:DAW,community:Discord,community:GitHub Issues,conditioning:MIDI,conditioning:audio,disentanglement:loudness,disentanglement:pitch,parameters:latent manipulation" data-usability="100"><td class="name-cell"><div class="model-name">DDSP-VST</div><div class="affiliation">Google Magenta</div></td><td class="partial data-cell" title="The model can be effectively trained on a consumer-grade GPU (the authors suggest ~2-3 hours on Colabâ€™s free tier). CPU-only training is not presented as a practical option.">~</td><td class="high data-cell" title="The model is explicitly designed for small, domain-specific datasets. Official documentation recommends 10â€“20 minutes of monophonic audio to train a usable instrument model, making personal recordings fully viable for musicians.">âœ”ï¸Ž</td><td class="partial data-cell" title="Complete training code and documented data preprocessing pipelines are provided, including an official Colab notebook for end-to-end model adaptation. However, these pathways rely on notebooks and codebases that have not been actively maintained in recent years, which may require additional setup or troubleshooting.">~</td><td class="partial data-cell" title="A streamlined Colab notebook with step-by-step instructions is provided, lowering the barrier for users with basic technical familiarity. However, no graphical training interface exists, and successful adaptation still requires navigating notebooks, file systems, and model formats, placing it beyond non-technical musicians.">~</td><td class="high data-cell" title="The code and trained models are released under the Apache License 2.0, which explicitly permits redistribution of modified and derived works, including trained and fine-tuned models, for both commercial and non-commercial use, provided attribution and license terms are preserved.">âœ”ï¸Ž</td><td class="high data-cell" title="DDSP-VST is distributed as native VST3 and AU plugins with a polished graphical interface, fully compatible with major DAWs. No custom setup is required beyond standard plugin installation.">âœ”ï¸Ž</td><td class="high data-cell" title="The plugins, pretrained models, training notebooks, and source code are freely available with no paywalls, subscriptions, or usage limits.">âœ”ï¸Ž</td><td class="high data-cell" title="The system is designed for real-time audio processing and synthesis within DAWs. A fixed 64 ms algorithmic latency is introduced due to frame-based pitch detection, which is standard for pitch-aware audio plugins and remains fully suitable for live and interactive performance.">âœ”ï¸Ž</td><td class="high data-cell" title="DDSP-VST integrates directly into standard music production workflows as both a real-time audio effect and a MIDI-controlled synthesizer, with automation support and DAW-level modulation (e.g., LFO assignment to pitch and gain).">âœ”ï¸Ž</td><td class="high data-cell" title="The project does not impose any additional licensing restrictions on generated audio. Outputs can be freely used for personal and commercial purposes without attribution requirements.">âœ”ï¸Ž</td><td class="high data-cell" title="An active Discord community is provided alongside GitHub issues, offering musician-oriented support, discussion, and troubleshooting.">âœ”ï¸Ž</td><td class="high data-cell" title="The model accepts multiple musically meaningful conditioning inputs, including audio and MIDI, which directly affect synthesis behavior rather than acting as descriptive prompts.">âœ”ï¸Ž</td><td class="high data-cell" title="DDSP-VST supports sample-accurate, continuous time-varying control over pitch, loudness, envelopes, and excitation signals, enabling fine-grained expressive manipulation suitable for performance and automation.">âœ”ï¸Ž</td><td class="high data-cell" title="Disentanglement is core to the DDSP architecture. Pitch and loudness are explicitly modeled as independent control pathways, resulting in predictable and interpretable behavior. Timbre, while not exposed as a conditioning input or runtime control, is treated as a disentangled inductive bias and is implicitly determined by the data used to train the model.">âœ”ï¸Ž</td><td class="high data-cell" title="The system exposes both high-level musical parameters (e.g., ADSR envelopes, gain, reverb) and low-level latent control (harmonicâ€“noise balance, pitch, and loudness trajectories). Core internal representations are directly and meaningfully manipulable, enabling precise and interpretable control over synthesis behavior.">âœ”ï¸Ž</td></tr>
<tr class="row-a" data-adaptability="50" data-affiliation="IRCAM" data-applications="MIDI-to-audio,audio synthesis,style transfer" data-controllability="100" data-name="AFTER" data-overall="78" data-tags="adaptation:pretrained codec,adaptation:training,technical:CLI,interface:Max/MSP,interface:Max4Live,interface:PureData,realtime:real-time,workflow:DAW,workflow:visual programming,community:GitHub Issues,conditioning:MIDI,conditioning:audio,disentanglement:structure,disentanglement:timbre,parameters:conditioning strength,parameters:diffusion steps,parameters:latent manipulation" data-usability="83"><td class="name-cell"><div class="model-name">AFTER</div><div class="affiliation">IRCAM</div></td><td class="partial data-cell" title="Training and export pipelines are GPU-oriented in practice. CPU-only training is technically possible but impractical within a reasonable timeframe.">~</td><td class="partial data-cell" title="The original version of the model, as described in the paper, was trained using approximately 400h of audio from the Synthesized Lakh dataset. Recent versions of AFTER include instrument-specific models, suggesting these data requirements have been significantly reduced. However, no documentation demonstrates that AFTER can be trained effectively on relatively small datasets.">~</td><td class="high data-cell" title="Complete training workflows are provided, including dataset preparation, autoencoder training (optional), diffusion training, and export for real-time inference. Pretrained audio codecs are available to skip the autoencoder stage.">âœ”ï¸Ž</td><td class="low data-cell" title="Training and export are conducted exclusively through command-line interfaces. Although documentation is thorough and aimed at technical users, adaptation requires significant programming knowledge, familiarity with PyTorch-based workflows, and manual configuration across multiple training stages.">âœ˜</td><td class="partial data-cell" title="The CC BY-NC 4.0 license permits sharing adapted models or checkpoints for non-commercial use with attribution, but redistribution is constrained and not explicitly promoted by the project.">~</td><td class="high data-cell" title="Inference is available through Max/MSP and Pure Data patches, as well as Max for Live devices for Ableton Live, all built on top of nn~. While no standalone VST or web-based interface is provided, the Max for Live device enables direct use within a DAW environment.">âœ”ï¸Ž</td><td class="high data-cell" title="The model, inference patches, and pretrained checkpoints are openly available. Inference can be run locally without usage limits, subscriptions, or paywalls once the software environment is set up.">âœ”ï¸Ž</td><td class="high data-cell" title="AFTER is explicitly designed for real-time audio generation. Optimized exports enable low-latency performance suitable for live use, including continuous audio-to-audio and MIDI-to-audio generation.">âœ”ï¸Ž</td><td class="high data-cell" title="AFTER integrates directly into established music workflows through visual programming environments and DAW-based contexts, specifically as a Max for Live device within Ableton Live, and is designed for live performance, real-time exploration, and structured musical interaction.">âœ”ï¸Ž</td><td class="partial data-cell" title="The generated output is licensed under Creative Commons Attributionâ€“NonCommercial 4.0. Use is permitted for personal, artistic, and research purposes, but commercial use is explicitly restricted, limiting professional adoption without additional permission.">~</td><td class="partial data-cell" title="User support is limited to GitHub Issues, which are primarily developer- and research-oriented. No dedicated musician-facing community or discussion forum is provided.">~</td><td class="high data-cell" title="AFTER supports multiple conditioning modalities, combining audio-to-audio and MIDI-to-audio pipelines for structural control, alongside audio conditioning for timbre.">âœ”ï¸Ž</td><td class="high data-cell" title="The model provides explicit, structured time-varying control by separating timbre and structure representations. Conditioning inputs (audio or MIDI) directly shape temporal evolution during generation, enabling fine-grained control over musical form and dynamics.">âœ”ï¸Ž</td><td class="high data-cell" title="AFTER explicitly disentangles timbre and structure through separate latent representations. This separation is central to the modelâ€™s design and enables predictable manipulation of distinct musical attributes.">âœ”ï¸Ž</td><td class="high data-cell" title="AFTER exposes several inference-time control parameters, including conditioning strength, diffusion step count, and latent space controls. The Max for Live devices support coarse latent exploration via a learned 2D timbre map, with optional refinement through direct manipulation of latent dimensions.">âœ”ï¸Ž</td></tr>
<tr class="row-a" data-adaptability="70" data-affiliation="IRCAM" data-applications="audio synthesis,style transfer" data-controllability="62" data-name="RAVE" data-overall="75" data-tags="dataset:small dataset,adaptation:prior training,adaptation:training,technical:CLI,technical:Colab,technical:tutorial,interface:AU,interface:Max/MSP,interface:PureData,interface:VST,realtime:real-time,workflow:DAW,workflow:visual programming,community:Discord,community:Forum,community:GitHub Discussions,community:GitHub Issues,conditioning:audio,parameters:latent manipulation,parameters:latent noise,parameters:latent prior" data-usability="92"><td class="name-cell"><div class="model-name">RAVE</div><div class="affiliation">IRCAM</div></td><td class="partial data-cell" title="A consumer-grade GPU (e.g., 8â€“16GB VRAM) is needed depending on configuration. Lightweight options (e.g., ONNX, Raspberry) reduce demands, but CPU-only training is not recommended.">~</td><td class="high data-cell" title="The model is designed to be trained on relatively small, domain-specific datasets, with documentation suggesting a few hours of homogeneous audio as a practical minimum, making personal datasets viable.">âœ”ï¸Ž</td><td class="high data-cell" title="Complete dataset preparation, training, export, and prior training pipelines are provided, with multiple architectures and configurations explicitly intended for adaptation on new data.">âœ”ï¸Ž</td><td class="partial data-cell" title="Training and preprocessing rely on command-line tools and Python environments. A Colab notebook, a detailed tutorial, and extensive documentation are provided, making adaptation accessible to users with basic technical skills, but no dedicated training GUI is available.">~</td><td class="partial data-cell" title="RAVEâ€™s CC BY-NC 4.0 license permits redistribution of trained models or checkpoints with attribution for non-commercial use. Sharing is legally allowed but commercially restricted and not explicitly structured or encouraged beyond general community practice.">~</td><td class="high data-cell" title="Inference is available via multiple user-facing interfaces, including a VST plugin and real-time integrations for Max/MSP and Pure Data, in addition to command-line tools.">âœ”ï¸Ž</td><td class="high data-cell" title="The modelâ€™s source code is publicly available, along with inference scripts and pretrained models. No login, subscription, or usage limits apply.">âœ”ï¸Ž</td><td class="high data-cell" title="The model supports low-latency, real-time audio synthesis and streaming, including CPU-based real-time inference suitable for live performance.">âœ”ï¸Ž</td><td class="high data-cell" title="RAVE can be embedded directly into common music workflows, including DAWs and visual programming environments, enabling both studio and live use.">âœ”ï¸Ž</td><td class="partial data-cell" title="The output may be used, modified, and shared for non-commercial purposes only, and attribution is required. Commercial use of generated material is not permitted under CC BY-NC 4.0.">~</td><td class="high data-cell" title="Active and accessible support is available via a dedicated Discord server, GitHub discussions, and regularly updated tutorials and examples.">âœ”ï¸Ž</td><td class="partial data-cell" title="RAVE primarily operates in an audio-to-audio paradigm. Conditioning is based on incoming audio, with no native support for text, MIDI, or symbolic inputs.">~</td><td class="partial data-cell" title="RAVE preserves temporal structure through audio-to-audio conditioning, with output following the time evolution of the input signal. However, it does not support explicit symbolic or structured time-varying controls (e.g. MIDI, aligned control envelopes, or temporal embeddings).">~</td><td class="partial data-cell" title="Post-training analysis identifies informative latent dimensions and improves controllability, but these dimensions are not associated with explicit or interpretable musical attributes, such as pitch or timbre.">~</td><td class="high data-cell" title="RAVE exposes a rich set of inference-time control parameters, including per-dimension latent scaling and bias, controllable noise injection, and optional latent priors, enabling fine-grained manipulation during synthesis.">âœ”ï¸Ž</td></tr>
<tr class="row-a" data-adaptability="80" data-affiliation="Neutone Inc." data-applications="audio synthesis,style transfer" data-controllability="50" data-name="Neutone Morpho" data-overall="74" data-tags="hardware:CPU,dataset:small dataset,adaptation:training,technical:GUI,technical:drag-and-drop,interface:AU,interface:VST,realtime:real-time,workflow:DAW,workflow:hardware,community:Discord,community:Help center,conditioning:audio,parameters:conditioning strength,parameters:randomness" data-usability="92"><td class="name-cell"><div class="model-name">Neutone Morpho</div><div class="affiliation">Neutone Inc.</div></td><td class="high data-cell" title="Custom model training does not require local compute resources. Morphoâ€™s training pipeline is fully cloud-based, requiring no GPU, no local installation, and no ML environment, which removes hardware barriers entirely for end users.">âœ”ï¸Ž</td><td class="high data-cell" title="Morpho is designed to be trained on small, musician-scale datasets, with clear guidance suggesting ~45 minutes of unique audio as a practical target. This scale is well within reach of individual musicians working with personal recordings or sound libraries.">âœ”ï¸Ž</td><td class="partial data-cell" title="A complete and practical adaptation pathway is provided through Neutoneâ€™s proprietary training service, allowing musicians to upload audio and train custom models end-to-end. However, the training process is closed-source and platform-bound, with no access to underlying training code, checkpoints, or local fine-tuning workflows.">~</td><td class="high data-cell" title="Model adaptation is designed explicitly for non-technical users. Training is performed via a drag-and-drop, no-code interface, with extensive musician-oriented guidance on dataset preparation, making the process accessible to users without programming or machine learning experience.">âœ”ï¸Ž</td><td class="partial data-cell" title="Trained models can be freely used by their creators within the Neutone ecosystem, but redistribution is constrained to the platform. Models cannot be exported or reused outside Neutoneâ€™s plugin environment, limiting portability despite creative freedom within the system.">~</td><td class="high data-cell" title="Neutone Morpho is distributed as a polished VST3/AU plugin with a dedicated graphical interface, designed for immediate use inside major DAWs. No scripting, external inference setup, or technical configuration is required.">âœ”ï¸Ž</td><td class="partial data-cell" title="The plugin itself is freely downloadable with a limited set of bundled models and a time-limited trial of the full library. Access to additional pretrained models and custom model training requires account creation and paid purchases, introducing moderate platform-level restrictions.">~</td><td class="high data-cell" title="Morpho is explicitly designed for real-time audio processing, operating with low and predictable latency suitable for live performance. The system is optimized for consumer CPUs (e.g., Apple M1-class hardware) and is demonstrated in both DAW and embedded hardware contexts.">âœ”ï¸Ž</td><td class="high data-cell" title="Morpho integrates directly into standard music production workflows as an audio effect and instrument plugin, with automation-ready parameters and compatibility with all major DAWs. Ongoing work (e.g., Project LYDIA with Roland) further demonstrates its suitability for hardware and live performance contexts.">âœ”ï¸Ž</td><td class="high data-cell" title="Audio output generated using Neutone Morpho is fully usable for personal and commercial purposes. Models are trained exclusively on licensed or user-provided data, and users retain ownership of sounds they create.">âœ”ï¸Ž</td><td class="high data-cell" title="Neutone maintains an active Discord community, direct support channels, detailed blog posts, FAQs, and ongoing artist-focused communication, providing accessible support for both creative and technical questions.">âœ”ï¸Ž</td><td class="partial data-cell" title="Morpho conditions on incoming audio, using learned timbral representations to transform signals in real time. While rich in effect, conditioning is limited to a single musically meaningful modality rather than multiple symbolic or structured inputs.">~</td><td class="partial data-cell" title="Time-localized control emerges implicitly through real-time resynthesis, as the output dynamically follows the temporal structure of the input signal. However, the model does not expose explicit time-indexed representations or independent control over temporal musical attributes (e.g., pitch curves or structural markers).">~</td><td class="partial data-cell" title="Neutone Morpho exhibits partial feature disentanglement through real-time audio resynthesis, where temporal structure and pitch contours of the input signal are largely preserved while timbre is transformed by the selected model. This yields predictable and musically coherent behavior in practice, but control pathways are implicit and dependent on the trained model, with no explicit or independently manipulable representations of pitch, loudness, or structure.">~</td><td class="partial data-cell" title="Neutone Morpho exposes a small set of learned macro-controls that influence stochastic behavior and the strength of coupling between input and output. These parameters offer musically intuitive control over unpredictability and responsiveness, but do not provide direct access to or manipulation of internal latent representations.">~</td></tr>
<tr class="row-a" data-adaptability="40" data-affiliation="The Hong Kong University of Science and Technology (HKUST) and MAP" data-applications="continuation,full song generation,lyrics-to-song" data-controllability="50" data-name="YuE" data-overall="49" data-tags="adaptation:LoRA,adaptation:fine-tuning,adaptation:pretrained checkpoints,technical:CLI,interface:Colab,interface:Gradio,community:Discord,community:GitHub Issues,conditioning:audio,conditioning:lyrics,conditioning:section,conditioning:style tags,conditioning:text,parameters:randomness,parameters:variability" data-usability="58"><td class="name-cell"><div class="model-name">YuE</div><div class="affiliation">The Hong Kong University of Science and Technology (HKUST) and MAP</div></td><td class="low data-cell" title="YuE requires dedicated high-end GPU infrastructure for training and scaling (e.g., tens to hundreds of NVIDIA H800 GPUs via Megatron-LM). Fine-tuning setup assumes substantial VRAM and distributed tooling. Therefore, training or fine-tuning is not feasible without institutional-level computational resources. ">âœ˜</td><td class="low data-cell" title="YuE was trained on large-scale datasets (e.g., hundreds of thousands of hours of audio), and does not support adaptation on small datasets. ">âœ˜</td><td class="high data-cell" title="YuE provides fine-tuning code in a subdirectory of the main GitHub repository. However, the pathway documentation assumes prior technical knowledge and infrastructure, which makes adaptation technically possible but not streamlined from a musician's perspective.  ">âœ”ï¸Ž</td><td class="low data-cell" title="Fine-tuning is provided only through low-level training scripts. No graphical interface, simplified notebook, or musician-oriented workflow is provided, resulting in high technical barriers to adaptation.">âœ˜</td><td class="high data-cell" title="YuE is released under the Apache 2.0 license, which explicitly permits modification and redistribution of derivative works, including fine-tuned models. Although fine-tuning is technically demanding, adapted checkpoints can be legally redistributed without restriction.">âœ”ï¸Ž</td><td class="partial data-cell" title="Multiple community-developed Gradio-based UIs are available for YuE, most of them based on or derived from the interface developed by Alisson Pereira Anjos (alisson-anjos). There is also a third-party one-click installer (Pinokio), which can simplify local setup on Windows. Still, these UIs require local setup, GPU configuration, and technical familiarity. Moreover, there are some community-develop Colab notebooks for specific tasks, such as music continuation. No official standalone or plug-and-play application in provided.">~</td><td class="high data-cell" title="YuEâ€™s inference code and pretrained checkpoints are openly available without paywalls, subscriptions, or usage limits. The model is released under the Apache 2.0 license, allowing unrestricted use, with no login or platform-specific access requirements.">âœ”ï¸Ž</td><td class="low data-cell" title="YuE does not support real-time or interactive generation. Inference requires a GPU, and generation latency is substantial (e.g., generating 30 seconds of audio takes ~150 s on a NVIDIA H800 and ~360 s on a RTX4090). ">âœ˜</td><td class="low data-cell" title="Interaction is limited to offline, file-based generation via scripts or standalone UIs, with no native support for DAW plugins, MIDI/OSC control, or other direct integration into existing creative setups.">âœ˜</td><td class="high data-cell" title="YuE outputs may be used for both personal and commercial purposes, and users retain ownership of generated content. However, the license encourages attribution to the model (â€œYuE by HKUST/M-A-Pâ€) and recommends transparency when publishing AI-generated works. ">âœ”ï¸Ž</td><td class="high data-cell" title="Support exists via a Discord channel, GitHub issues and documentation. ">âœ”ï¸Ž</td><td class="partial data-cell" title="YuE supports generation conditioning through text (lyrics, structural labels, and style tags), and optional reference audio for in-context learning. ">~</td><td class="partial data-cell" title="YuE provides limited time-localized control through section-based generation, allowing different musical behaviors across structured segments (e.g., verse, chorus). However, control remains discrete and symbolic, with no fine-grained or continuous time-varying manipulation within segments.">~</td><td class="partial data-cell" title="YuE introduces training-stage mechanisms to reduce entanglement between textual and audio conditioning, yielding partial disentanglement at inference time. Nonetheless, internal musical attributes remain bundled, and disentanglement is neither explicit nor robustly exposed to user-level control.">~</td><td class="partial data-cell" title="YuE supports a few inference-time control parameters, limited to repetition penalty (to discourage looping) and deterministic seeding. Other controls, including temperature, top-p sampling and per-segment guidance scaling, remain hard-coded in the inference script, and direct manipulation of internal latent representations is not supported.">~</td></tr>
<tr class="row-a" data-adaptability="40" data-affiliation="Singapore University of Technology and Design and Lamda Labs. " data-applications="full song generation,lyrics-to-song" data-controllability="50" data-name="JAM" data-overall="44" data-tags="adaptation:fine-tuning,adaptation:pretrained checkpoints,adaptation:training,technical:CLI,interface:Gradio,community:GitHub Issues,conditioning:audio,conditioning:lyrics,conditioning:text,conditioning:timing,parameters:conditioning strength,parameters:diffusion steps,parameters:duration" data-usability="42"><td class="name-cell"><div class="model-name">JAM</div><div class="affiliation">Singapore University of Technology and Design and Lamda Labs. </div></td><td class="partial data-cell" title="The model requires a CUDA-compatible GPU with sufficient VRAM (+8GB is recommended). Not feasible to train the model on CPU.">~</td><td class="low data-cell" title="No clear information about the dataset size is provided. The model was trained on a dataset of ~54k hours of audio, which makes adaptation infeasible with musicianâ€™s own data. ">âœ˜</td><td class="high data-cell" title="JAM provides complete code and documentation for pretraining, supervised fine-tuning, and direct preference optimization. Model checkpoints are available. ">âœ”ï¸Ž</td><td class="low data-cell" title="Adaptation requires technical expertise. All training a preprocessing rely on command-line implementation. No user-friendly adaption interface is provided. ">âœ˜</td><td class="partial data-cell" title="The use, modification and distribution of JAM is subject to Stability AI Community License Agreement, which is restricted to research and non-commercial use, and requires attribution and notice. Commercial-use may be obtained by registering with Stability AI or with a separate commercial license.">~</td><td class="partial data-cell" title="There is a hosted HuggingFace web demo, but it is currently unavailable. However, the Gradio code is accessible and can be run locally, though it requires installation and environment setup by the user.">~</td><td class="high data-cell" title="JAMâ€™s source code is openly available and can be freely used without limits, paywalls, or subscriptions. ">âœ”ï¸Ž</td><td class="low data-cell" title="No clear information about the time required for generation is provided. No reference to real-time generation is provided in the documentation or the research article. ">âœ˜</td><td class="low data-cell" title="There are no clear instructions or specific tools to integrate JAM to a musicianâ€™s workflow. ">âœ˜</td><td class="partial data-cell" title="Outputs generated with JAM cannot be used for commercial-use, and must not be including content that violates copyright laws. Responsibility of the generated outputs relies entirely with the end user. ">~</td><td class="partial data-cell" title="Support exists via GitHub issues and documentation, but there is no dedicated musician-oriented support space.">~</td><td class="partial data-cell" title="JAM can be conditioned by lyrics (including word- and phoneme-level timing) and style (via text or audio).">~</td><td class="partial data-cell" title="JAM supports precise time-varying control over vocal structure, with explicit word- and phoneme-level timing and duration inputs that directly guide generation at the latent space. However, no time-varying control is provided for the accompaniment or instrumental structure.">~</td><td class="partial data-cell" title="JAM separates lyrics timing, global duration, and style conditioning through explicit control pathways, enabling partial disentanglement of vocal structure. However, musical attributes are not independently controllable, and style remains an entangled representation, limiting fully interpretable, attribute-specific control.">~</td><td class="partial data-cell" title="JAM provides several generation control parameters, including global duration, adjustable guidance strength, and diffusion step configuration, allowing users to directly influence generation behavior. However, direct manipulation of internal representations is not supported.">~</td></tr>
<tr class="row-a" data-adaptability="40" data-affiliation="Stablility AI" data-applications="style transfer,text-to-music" data-controllability="38" data-name="Stable Audio Open Small" data-overall="43" data-tags="adaptation:fine-tuning,adaptation:pretrained checkpoints,adaptation:training,technical:CLI,interface:Gradio,community:Discord,community:GitHub Issues,conditioning:audio,conditioning:text,conditioning:timing,parameters:conditioning strength,parameters:diffusion steps,parameters:duration,parameters:randomness,parameters:sampling strategy" data-usability="50"><td class="name-cell"><div class="model-name">Stable Audio Open Small</div><div class="affiliation">Stablility AI</div></td><td class="partial data-cell" title="Stable Audio Open Small research paper reports using institutional-scale hardware for fine-tuning (8 x H100 GPUS). However, the released source code supports adaptation on a single consumer-grade GPU, although with significant memory and storage demands. Adapting the model with CPU-only is impractical, and larger GPU setups will significantly improve training and fine-tuning feasibility. ">~</td><td class="low data-cell" title="Stable Audio Open Small training relies on large-scale curated data (i.e., thousands of hours of audio). No documentation suggests that effective training or fine-tuning can be achieved with small personal datasets. ">âœ˜</td><td class="high data-cell" title="Stable Audio Open provides complete training and fine-tuning code, supports continuation from pretrained checkpoints, and includes documented data and model configuration pipelines. This makes adaptation on custom datasets technically feasible, even if demanding in practice. ">âœ”ï¸Ž</td><td class="low data-cell" title="Adaptation requires substantial technical expertise, including configuration of diffusion training pipelines, dataset preparation, and GPU-based training. No user-friendly interface for model adaptation is provided">âœ˜</td><td class="partial data-cell" title="Redistribution of adapted models is permitted under the Stability AI Community License, but subject to licensing conditions and commercial revenue thresholds. ">~</td><td class="partial data-cell" title="The model can be run through a basic Gradio UI provided in the stable-audio-tools library, offering a simplified interactive interface. However, it requires installation and environment setup by the user.">~</td><td class="partial data-cell" title="Stable Audio Open Small can be run locally with open pretrained checkpoints and inference code without paywalls, usage limits, or subscriptions, subject only to license agreement acceptance.">~</td><td class="partial data-cell" title="Stable Audio Open Small can be interactive on high-end consumer GPU for short audio segments (e.g., achieving sub-200ms response time), but it not designed for low-latency or streaming use on CPU and is impractical for live performance on usual personal hardware. ">~</td><td class="low data-cell" title="Although optimized for low-latency generation on GPU, this model does not offer native integration with DAWs, live music environments, visual programming systems, or musical hardware. Usage is limited to file- or script-based generation via research tooling, with no direct embedding in existing creative workflows">âœ˜</td><td class="partial data-cell" title="Generated outputs are owned by the user and may be used for both non-commercial and commercial purposes. However, commercial use is conditional on registration and subject to revenue thresholds, attribution requirements, and restrictions on downstream uses (e.g., training other foundational models). That is, output usage is permitted but not unrestricted.">~</td><td class="high data-cell" title="Support exists via a Discord channel (i.e., specific sub-channel â€œstable-audioâ€), GitHub issues and documentation. ">âœ”ï¸Ž</td><td class="partial data-cell" title="Stable Audio Open Small primarily conditions on text prompts. No additional musically structured modalities (e.g., MIDI, symbolic control) are available. In addition to text prompts, the model supports audio-to-audio generation by initializing the diffusion process with a reference recording, enabling implicit style transfer without additional training. ">~</td><td class="partial data-cell" title="The model enables time-localized influence through audio initialization (e.g., beat-aligned or voice-guided generation), but does not provide explicit or fine-grained time-varying control over musical attributes.">~</td><td class="low data-cell" title="No disentangled or explicitly separable musical controls are provided. Musical attributes are implicitly entangled within prompt- and audio-based generation.">âœ˜</td><td class="partial data-cell" title="Stable Audio Open Small provides several low-level inference parameters (e.g., duration, diffusion steps, standard diffusion cfg values), allowing certain degree of generation tuning. However, it does not provide direct manipulation of internal representations or fine-grained controls for musical attributes.">~</td></tr>
<tr class="row-a" data-adaptability="30" data-affiliation="Meta AI" data-applications="continuation,text-to-music" data-controllability="50" data-name="MusicGen" data-overall="43" data-tags="adaptation:fine-tuning,adaptation:pretrained checkpoints,adaptation:training,technical:CLI,interface:Colab,interface:Gradio,community:GitHub Issues,conditioning:melody,conditioning:text,parameters:conditioning strength,parameters:duration,parameters:randomness,parameters:sampling strategy" data-usability="50"><td class="name-cell"><div class="model-name">MusicGen</div><div class="affiliation">Meta AI</div></td><td class="low data-cell" title="Training and fine-tuning require institutional-scale hardware. Official models were trained using dozens of GPUs, and even inference for medium/large variants requires ~16 GB VRAM GPUs. CPU-only training is not supported.">âœ˜</td><td class="low data-cell" title="The model is designed around very large curated datasets (â‰ˆ20k hours). Fine-tuning presupposes similarly structured datasets with metadata, which makes the use of personal data collections impractical.">âœ˜</td><td class="high data-cell" title="MusicGen provides complete training and fine-tuning code, supports continuation from pretrained checkpoints, and includes documented data processing pipelines. This makes adaptation on personal or custom datasets technically feasible, even if demanding in practice.">âœ”ï¸Ž</td><td class="low data-cell" title="Adaptation requires extensive technical expertise (e.g,., Dora, AudioCraft configs, tokenizer alignment, distributed training). No user-friendly adaptation interface is provided.">âœ˜</td><td class="partial data-cell" title="While the software is permissively licensed (MIT), redistribution of trained models and checkpoints is constrained by the weights license (CC BY-NC 4.0), which imposes a non-commercial restriction and requires attribution.">~</td><td class="partial data-cell" title="Gradio code and Colab notebooks are provided, but require setup and GPU access. A hosted HuggingFace web demo is available but currently unreliable.">~</td><td class="high data-cell" title="Inference via HuggingFace Spaces, although currently unreliable, is freely accessible without login or usage restrictions. Pretrained checkpoints and inference code are publicly available, with no paywalls, subscriptions, or usage limits imposed on model use.">âœ”ï¸Ž</td><td class="low data-cell" title="Generation is offline and not suitable for real-time use. Local inference requires a GPU.">âœ˜</td><td class="low data-cell" title="MusicGen offers no native integration with DAWs, live music environments, or hardware. Usage is limited to file-based generation via notebooks or scripts.">âœ˜</td><td class="high data-cell" title="Generated audio can be used for personal and commercial purposes under the modelâ€™s license.">âœ”ï¸Ž</td><td class="partial data-cell" title="Support exists via GitHub issues, documentation, and community tutorials, but there is no dedicated musician-oriented support space.">~</td><td class="partial data-cell" title="Supports text conditioning and melody conditioning via chromagram (audio-derived). No MIDI or symbolic score input is supported.">~</td><td class="partial data-cell" title="Melody conditioning enables coarse time-aligned control over harmonic structure, but there is no per-frame or parameter-level temporal control exposed to users.">~</td><td class="partial data-cell" title="Text and melody controls influence different aspects of generation, but interactions remain implicit and do not enable interpretable or musically isolated guidance.">~</td><td class="partial data-cell" title="MusicGen exposes high-level generation parameters such as duration, sampling controls (temperature, top-k/top-p), and a classifier-free guidance (CFG) coefficient. These provide coarse control over variability and conditioning strength, but do not enable manipulation of internal representations.">~</td></tr>
<tr class="row-a" data-adaptability="0" data-affiliation="Suno, Inc." data-applications="continuation,editing,full song generation,remixing,text-to-music" data-controllability="25" data-name="Suno" data-overall="28" data-tags="interface:Android app,interface:iOS app,interface:web UI,community:Discord,community:Help center,conditioning:audio,conditioning:lyrics,conditioning:style tags,conditioning:text,parameters:conditioning strength,parameters:randomness" data-usability="58"><td class="name-cell"><div class="model-name">Suno</div><div class="affiliation">Suno, Inc.</div></td><td class="low data-cell" title="No information is provided regarding the hardware requirements for training the model.">âœ˜</td><td class="low data-cell" title="No information is provided about the amount or type of data required for training or fine-tuning the model.">âœ˜</td><td class="low data-cell" title="No training or fine-tuning pathways are exposed to users (e.g., no code, checkpoints, or interfaces), making adaptation infeasible from a musicianâ€™s perspective.">âœ˜</td><td class="low data-cell" title="Technical barriers for adaptation cannot be evaluated, as no adaptation mechanisms are provided or documented.">âœ˜</td><td class="low data-cell" title="Model weights and checkpoints are not accessible to users. The system is provided solely as a hosted service, and redistribution or sharing of adapted models is not permitted under the platformâ€™s terms.">âœ˜</td><td class="high data-cell" title="A dedicated consumer-facing interface is provided for music generation through a web-based application, requiring no local installation or technical setup. Additional iOs and Android apps are available.">âœ”ï¸Ž</td><td class="partial data-cell" title="Access requires user accounts and is subject to usage constraints, such as quotas, plan-dependent features, or queue-based generation, which may limit continuous or unrestricted use.">~</td><td class="partial data-cell" title="Generation occurs with noticeable latency and is not designed for live or audio-rate real-time interaction. The system supports interactive use but not real-time performance or streaming control.">~</td><td class="low data-cell" title="The system operates as a standalone platform. Although generated audio can be exported, there is no supported mechanism for integrating the model into DAWs, live music environments, or other existing music workflows.">âœ˜</td><td class="partial data-cell" title="Some use limitations apply to the generated output (e.g., free-tier outputs are non-commercial and require attribution; commercial use is tied to paid tiers; additional restrictions include prohibitions on competitive use and using output to train other ML models).">~</td><td class="high data-cell" title="User-facing support resources are available via a help center and a community Discord.">âœ”ï¸Ž</td><td class="partial data-cell" title="Suno primarily relies on text-based conditioning, including free-form prompts and lyrics. It also allows reference-based reuse of stylistic characteristics from existing songs, enabling style guidance through audio examples without exposing explicit or interpretable conditioning representations.">~</td><td class="low data-cell" title="Suno does not expose explicit time-varying control signals such as aligned envelopes, symbolic sequences, or structured temporal representations.">âœ˜</td><td class="low data-cell" title="No disentangled or explicitly separable musical control dimensions are exposed. Attributes such as timbre, harmony, rhythm, and form are implicitly entangled within prompt-based generation.">âœ˜</td><td class="partial data-cell" title="Suno exposes a small set of high-level inference-time controls, such as variability and conditioning strength for style or audio inputs, through its interface. No access to low-level model parameters or internal representations is provided.">~</td></tr>
<tr class="row-a" data-adaptability="0" data-affiliation="Udio" data-applications="continuation,editing,full song generation,remixing,text-to-music" data-controllability="12" data-name="Udio" data-overall="21" data-tags="interface:iOS app,interface:web UI,community:Discord,community:Forum,community:Help center,conditioning:lyrics,conditioning:style tags,conditioning:text" data-usability="50"><td class="name-cell"><div class="model-name">Udio</div><div class="affiliation">Udio</div></td><td class="low data-cell" title="No information is available about the hardware requirements to train the model. ">âœ˜</td><td class="low data-cell" title="No information is available about the amount or type of data required to adapt the model to a musicianâ€™s own music. ">âœ˜</td><td class="low data-cell" title="No training, fine-tuning, or adaptation pathways are provided for users (e.g., no code, checkpoints, or interfaces), making adaptation infeasible from a musicianâ€™s perspective.">âœ˜</td><td class="low data-cell" title="Technical barriers for adaptation cannot be evaluated, as no adaptation mechanisms are provided or documented.">âœ˜</td><td class="low data-cell" title="Model weights and checkpoints are not accessible to users. The system is provided solely as a hosted service, and redistribution or sharing of adapted models is not permitted under the platformâ€™s terms.">âœ˜</td><td class="high data-cell" title="A dedicated consumer-facing interface is provided for music generation through a web-based application, requiring no local installation or technical setup. An iOS app is also available. ">âœ”ï¸Ž</td><td class="partial data-cell" title="Access requires using user accounts, accepting terms and conditions and is subject to usage constraints, such as quotas and plan-dependent features, which may limit continuous or unrestricted use. Free trials and promotional codes are available.">~</td><td class="partial data-cell" title="Generation takes several seconds and is not designed for live or audio-rate real-time interaction. The system supports real-time editing but not real-time performance or streaming control.">~</td><td class="low data-cell" title="The system operates as a standalone platform. There is no supported mechanism for integrating the model into DAWs, live music environments, or other existing music workflows.">âœ˜</td><td class="low data-cell" title="The modelâ€™s outputs are subject to proprietary licensing: use is restricted to personal, non-commercial purposes, downloading outputs is prohibited, and ownership of generated content is retained by the company, which constitutes a heavy restriction to output usage. Attribution is required for public use of outputs, although this requirement may be waived for outputs generated under a paid subscription.">âœ˜</td><td class="high data-cell" title="User-facing support resources are available via a help center and  chatbox in their website, and a community in discord Discord and a forum in Reddit, where musicians and music-lovers are encourage to share tips and support.">âœ”ï¸Ž</td><td class="partial data-cell" title="Udio relies on text-based conditioning, including free-form prompts and lyrics. It allows to set stylistic reference to set the overall mood and tempo of the track.">~</td><td class="low data-cell" title="Udio provides limited time-localized control through iterative generation and segment-based continuation. However, it does not support explicit or fine-grained temporal conditioning such as per-frame, per-beat, or symbolic time-aligned controls. As a result, the generation itself cannot be guided by meaningful temporal controls.">âœ˜</td><td class="low data-cell" title="No disentangled or explicitly separable musical control dimensions are exposed. Attributes such as timbre, harmony, rhythm, and form are implicitly entangled within prompt-based generation.">âœ˜</td><td class="low data-cell" title="Udio does not provide any control parameters beyond the conditioning inputs.">âœ˜</td></tr>
</tbody>
</table>
</div>
<div id="table-guide" style="margin-top: 15px;">
<h2 style="text-align: left; margin: 0 0 5px 0; font-size: 1.3em;">How to navigate this table?</h2>
<p>The MusGU+ framework evaluates models across 15 criteria distributed among three dimensions: <strong>Adaptability</strong> (5 criteria), <strong>Usability</strong> (6 criteria), and <strong>Controllability</strong> (4 criteria). Each criterion is evaluated on a three-level scale:
    <span class="legend-symbol" style="background: #c7f0d8; padding: 2px 8px; border-radius: 3px; font-size: 13px; color: #2d6a4f;">âœ”ï¸Ž</span> fully supported,
    <span class="legend-symbol" style="background: #ffd699; padding: 2px 8px; border-radius: 3px; font-size: 13px; color: #e67700;">~</span> partially supported, or
    <span class="legend-symbol" style="background: #ffb3ba; padding: 2px 8px; border-radius: 3px; font-size: 13px; color: #c1121f;">âœ˜</span> not supported.
  </p>
<p>The table includes <strong>interactive elements</strong>:</p>
<ul>
<li><strong>Hovering</strong> over a cell reveals a tooltip with the justification behind the assigned evaluation.</li>
<li><strong>Clicking</strong> on tag filters (in column headers or under Musical Applications) filters the table to show only models with those features.</li>
<li><strong>Dimension filters</strong> (â‰¥60% buttons) show models that score at least 60% in that dimension.</li>
<li><strong>Expand buttons</strong> (+N) in column headers reveal additional filter tags for that criterion.</li>
</ul>
<p>For a detailed breakdown of each model's evaluation, explore the corresponding YAML file in the <a href="https://github.com/lauraibnz/MusGU-plus/tree/main/projects" style="color: #0066cc;">projects folder</a>.</p>
</div>
<div id="musgo-relationship" style="padding: 1em 1.5em; margin: 5px auto 0; max-width: 1200px; font-size: 1em; line-height: 1.6;">
<h2 style="text-align: left; margin: 0 0 5px 0; font-size: 1.3em;">Relationship to MusGO</h2>
<p style="margin-top: 0; margin-bottom: 0; line-height: 1.6; font-size: 1em;">
    MusGU+ builds on insights from the <a href="https://roserbatlleroca.github.io/MusGO_framework/" style="color: #0066cc;" target="_blank">MusGO framework</a>. MusGO (Music-Generative Open AI) is an openness-focused evaluation framework for music-generative AI. While MusGO assesses transparency and responsible research practices, MusGU+ supports informed selection and practical adoption of generative music models by musicians.
  </p>
</div>
<div id="footer">
<p id="build-time">Table last built on 2026-02-03 at 12:40 UTC</p>
</div>
<script>
// Filter state
var activeFilters = new Set();
var activeTags = new Set();
var activeApplications = new Set();
var currentOpenPopup = null;

// Criterion ID to display name mapping
var criterionNames = {
  'hardware': 'Hardware Requirements',
  'dataset': 'Dataset Size',
  'adaptation': 'Adaptation Pathways',
  'technical': 'Technical Barriers',
  'redistribution': 'Model Redistribution',
  'interface': 'Interface Availability',
  'access': 'Access Restrictions',
  'realtime': 'Real-time Capabilities',
  'workflow': 'Workflow Integration',
  'licensing': 'Output Licensing',
  'community': 'Community Support',
  'conditioning': 'Conditioning Inputs',
  'timevarying': 'Time-Varying Control',
  'disentanglement': 'Feature Disentanglement',
  'parameters': 'Control Parameters'
};

// Dimension filter to display name mapping
var dimensionNames = {
  'adaptability': 'Adaptability',
  'usability': 'Usability',
  'controllability': 'Controllability'
};

// Custom search implementation
function initSearch() {
  var searchInput = document.querySelector('.search');
  if (searchInput) {
    searchInput.addEventListener('input', applyFilters);
  }
}

// Sorting functionality
var currentSort = { column: null, state: 0 };

function initSorting() {
  var sortableHeaders = document.querySelectorAll('.sortable');
  
  sortableHeaders.forEach(function(header) {
    header.addEventListener('click', function(e) {
      // Don't sort if clicking on dimension filter tag
      if (e.target.classList.contains('dimension-filter-tag')) {
        return;
      }
      
      var sortColumn = this.getAttribute('data-sort');
      var sortType = this.getAttribute('data-type');
      
      if (currentSort.column === sortColumn) {
        currentSort.state = (currentSort.state + 1) % 3;
      } else {
        currentSort.column = sortColumn;
        currentSort.state = 1;
      }
      
      if (currentSort.state === 0) {
        sortTable('overall', 'number', false);
        updateSortArrows(null, 0);
        currentSort.column = null;
      } else {
        var ascending = (currentSort.state === 2);
        sortTable(sortColumn, sortType, ascending);
        updateSortArrows(this, currentSort.state);
      }
    });
  });
}

function sortTable(column, type, ascending) {
  var tbody = document.querySelector('#musgu-table tbody');
  var rows = Array.from(tbody.querySelectorAll('tr'));
  
  rows.sort(function(a, b) {
    var aValue, bValue;
    
    if (type === 'number') {
      aValue = parseFloat(a.getAttribute('data-' + column)) || 0;
      bValue = parseFloat(b.getAttribute('data-' + column)) || 0;
    } else {
      aValue = (a.getAttribute('data-' + column) || '').toLowerCase();
      bValue = (b.getAttribute('data-' + column) || '').toLowerCase();
    }
    
    if (aValue < bValue) return ascending ? -1 : 1;
    if (aValue > bValue) return ascending ? 1 : -1;
    return 0;
  });
  
  rows.forEach(function(row) {
    tbody.appendChild(row);
  });
}

function updateSortArrows(activeHeader, state) {
  document.querySelectorAll('.sortable').forEach(function(header) {
    header.classList.remove('active');
    var arrow = header.querySelector('.sort-arrow');
    if (arrow) {
      arrow.textContent = 'â–´â–¾';
      arrow.style.opacity = '0.3';
    }
  });
  
  if (activeHeader && state !== 0) {
    activeHeader.classList.add('active');
    var arrow = activeHeader.querySelector('.sort-arrow');
    if (arrow) {
      arrow.style.opacity = '1';
    }
  }
}

// Filter functionality
function initFilters() {
  // Dimension filter tags (in table headers)
  var dimensionTags = document.querySelectorAll('.dimension-filter-tag');
  dimensionTags.forEach(function(tag) {
    tag.addEventListener('click', function(e) {
      e.stopPropagation();
      var filterName = this.getAttribute('data-filter');
      
      if (activeFilters.has(filterName)) {
        activeFilters.delete(filterName);
        this.classList.remove('active');
      } else {
        activeFilters.add(filterName);
        this.classList.add('active');
      }
      
      applyFilters();
      updateClearButton();
      updateActiveFiltersDisplay();
    });
  });
  
  // Criterion tag filters (inline under criterion names)
  var criterionTags = document.querySelectorAll('.criterion-tag');
  criterionTags.forEach(function(tag) {
    tag.addEventListener('click', function(e) {
      e.stopPropagation();
      var tagName = this.getAttribute('data-tag');
      var criterionId = this.getAttribute('data-criterion');
      var tagId = criterionId + ':' + tagName;
      
      if (activeTags.has(tagId)) {
        activeTags.delete(tagId);
        document.querySelectorAll('.criterion-tag[data-tag="' + tagName + '"][data-criterion="' + criterionId + '"]').forEach(function(t) {
          t.classList.remove('active');
        });
      } else {
        activeTags.add(tagId);
        document.querySelectorAll('.criterion-tag[data-tag="' + tagName + '"][data-criterion="' + criterionId + '"]').forEach(function(t) {
          t.classList.add('active');
        });
      }
      
      applyFilters();
      updateClearButton();
      updateActiveFiltersDisplay();
    });
  });
  
  var clearBtn = document.getElementById('clear-filters-inline');
  if (clearBtn) {
    clearBtn.addEventListener('click', clearFilters);
  }
}

function clearFilters() {
  activeFilters.clear();
  activeTags.clear();
  activeApplications.clear();
  
  document.querySelectorAll('.dimension-filter-tag').forEach(function(tag) {
    tag.classList.remove('active');
  });
  
  document.querySelectorAll('.criterion-tag').forEach(function(tag) {
    tag.classList.remove('active');
  });
  
  document.querySelectorAll('.application-tag').forEach(function(tag) {
    tag.classList.remove('active');
  });
  
  applyFilters();
  updateClearButton();
  updateActiveFiltersDisplay();
}

function updateClearButton() {
  var clearBtn = document.getElementById('clear-filters-inline');
  if (clearBtn) {
    if (activeFilters.size > 0 || activeTags.size > 0 || activeApplications.size > 0) {
      clearBtn.classList.add('visible');
    } else {
      clearBtn.classList.remove('visible');
    }
  }
}

function updateActiveFiltersDisplay() {
  var container = document.querySelector('.active-filters-tags');
  container.innerHTML = '';
  
  // Display dimension filters
  activeFilters.forEach(function(filterName) {
    var dimensionName = dimensionNames[filterName] || filterName;
    var tagEl = document.createElement('div');
    tagEl.className = 'active-filter-tag';
    tagEl.innerHTML = '<span>' + dimensionName + ' â‰¥60%</span><span class="remove-icon">âœ•</span>';
    tagEl.addEventListener('click', function() {
      removeDimensionFilter(filterName);
    });
    container.appendChild(tagEl);
  });
  
  // Count how many criteria each tag name appears in (across the entire table, not just active)
  var tagNameInMultipleCriteria = {};
  document.querySelectorAll('.criterion-tag').forEach(function(tag) {
    var tagName = tag.getAttribute('data-tag');
    var criterionId = tag.getAttribute('data-criterion');
    var key = tagName;
    
    if (!tagNameInMultipleCriteria[key]) {
      tagNameInMultipleCriteria[key] = new Set();
    }
    tagNameInMultipleCriteria[key].add(criterionId);
  });
  
  // Display criterion tags with criterion context only if tag name exists in multiple criteria
  activeTags.forEach(function(tagId) {
    var parts = tagId.split(':');
    var criterionId = parts[0];
    var tagName = parts[1];
    var criterionName = criterionNames[criterionId] || criterionId;
    
    var tagEl = document.createElement('div');
    tagEl.className = 'active-filter-tag';
    
    // Show criterion context only if this tag name exists in multiple criteria in the table
    var displayText = tagNameInMultipleCriteria[tagName] && tagNameInMultipleCriteria[tagName].size > 1
      ? tagName + ' (' + criterionName + ')' 
      : tagName;
    
    tagEl.innerHTML = '<span>' + displayText + '</span><span class="remove-icon">âœ•</span>';
    tagEl.addEventListener('click', function() {
      removeTag(tagId);
    });
    container.appendChild(tagEl);
  });
  
  // Display application filters
  activeApplications.forEach(function(appName) {
    var tagEl = document.createElement('div');
    tagEl.className = 'active-filter-tag';
    tagEl.innerHTML = '<span>' + appName + '</span><span class="remove-icon">âœ•</span>';
    tagEl.addEventListener('click', function() {
      removeApplicationFilter(appName);
    });
    container.appendChild(tagEl);
  });
}

function removeTag(tagId) {
  activeTags.delete(tagId);
  
  var parts = tagId.split(':');
  var criterionId = parts[0];
  var tagName = parts[1];
  
  document.querySelectorAll('.criterion-tag[data-tag="' + tagName + '"][data-criterion="' + criterionId + '"]').forEach(function(tag) {
    tag.classList.remove('active');
  });
  
  applyFilters();
  updateClearButton();
  updateActiveFiltersDisplay();
}

function removeDimensionFilter(filterName) {
  activeFilters.delete(filterName);
  
  document.querySelectorAll('.dimension-filter-tag[data-filter="' + filterName + '"]').forEach(function(tag) {
    tag.classList.remove('active');
  });
  
  applyFilters();
  updateClearButton();
  updateActiveFiltersDisplay();
}

function removeApplicationFilter(appName) {
  activeApplications.delete(appName);
  
  document.querySelectorAll('.application-tag[data-application="' + appName + '"]').forEach(function(tag) {
    tag.classList.remove('active');
  });
  
  applyFilters();
  updateClearButton();
  updateActiveFiltersDisplay();
}

function initApplicationTags() {
  var applicationTags = document.querySelectorAll('.application-tag');
  
  applicationTags.forEach(function(tag) {
    tag.addEventListener('click', function() {
      var appName = tag.getAttribute('data-application');
      
      if (tag.classList.contains('active')) {
        tag.classList.remove('active');
        activeApplications.delete(appName);
      } else {
        tag.classList.add('active');
        activeApplications.add(appName);
      }
      
      updateActiveFiltersDisplay();
      applyFilters();
      updateClearButton();
    });
  });
}

function applyFilters() {
  var searchTerm = document.querySelector('.search') ? document.querySelector('.search').value.toLowerCase() : '';
  var rows = document.querySelectorAll('#musgu-table tbody tr');
  
  rows.forEach(function(row) {
    var showRow = true;
    
    if (searchTerm) {
      var nameCell = row.querySelector('.name-cell');
      var orgCell = row.querySelector('.org');
      var nameText = nameCell ? nameCell.textContent.toLowerCase() : '';
      var orgText = orgCell ? orgCell.textContent.toLowerCase() : '';
      
      if (!nameText.includes(searchTerm) && !orgText.includes(searchTerm)) {
        showRow = false;
      }
    }
    
    if (showRow && activeFilters.size > 0) {
      var passesFilters = true;
      
      activeFilters.forEach(function(filterName) {
        var score = parseInt(row.getAttribute('data-' + filterName)) || 0;
        if (score < 60) {
          passesFilters = false;
        }
      });
      
      showRow = passesFilters;
    }
    
    if (showRow && activeTags.size > 0) {
      var rowTags = row.getAttribute('data-tags');
      if (!rowTags) {
        showRow = false;
      } else {
        var rowTagsArray = rowTags.split(',').map(function(t) { return t.trim(); });
        var hasAllTags = true;
        
        activeTags.forEach(function(tagName) {
          if (!rowTagsArray.includes(tagName)) {
            hasAllTags = false;
          }
        });
        
        showRow = hasAllTags;
      }
    }
    
    if (showRow && activeApplications.size > 0) {
      var rowApps = row.getAttribute('data-applications');
      if (!rowApps) {
        showRow = false;
      } else {
        var rowAppsArray = rowApps.split(',').map(function(a) { return a.trim(); });
        var hasAllApps = true;
        
        activeApplications.forEach(function(appName) {
          if (!rowAppsArray.includes(appName)) {
            hasAllApps = false;
          }
        });
        
        showRow = hasAllApps;
      }
    }
    
    // Use visibility instead of display to maintain column widths
    if (showRow) {
      row.style.display = '';
      row.style.visibility = '';
    } else {
      row.style.display = 'none';
      row.style.visibility = 'collapse';
    }
  });
}

window.addEventListener('DOMContentLoaded', function() {
  initSearch();
  initSorting();
  initFilters();
  initApplicationTags();
  initTagExpansion();
  
  // Show page after everything is initialized
  document.body.classList.add('ready');
});

function initTagExpansion() {
  var expandButtons = document.querySelectorAll('.expand-tags-btn');
  
  // Hide the entire table while calculating
  var table = document.getElementById('musgu-table');
  if (table) {
    table.classList.add('calculating');
  }
  
  // Wait for fonts to load before measuring
  if (document.fonts && document.fonts.ready) {
    document.fonts.ready.then(function() {
      calculateAndLockWidths();
    });
  } else {
    // Fallback if Font Loading API not available
    setTimeout(calculateAndLockWidths, 100);
  }
  
  function calculateAndLockWidths() {
    var containerMaxWidth = 100; // Reduced from 150 to make columns narrower
    var gap = 4;
    
    // Process each criterion column
    document.querySelectorAll('.criterion-tags').forEach(function(container) {
      var cell = container.closest('th');
      if (!cell) return;
      
      // Measure the criterion title width
      var titleElement = cell.querySelector('.criterion-header-wrapper > span');
      var titleWidth = 0;
      if (titleElement) {
        titleWidth = titleElement.offsetWidth;
      }
      
      var allTags = Array.from(container.querySelectorAll('.criterion-tag'));
      var expandBtn = container.querySelector('.expand-tags-btn');
      
      if (allTags.length === 0) {
        // No tags, just ensure cell is at least as wide as title
        var cellStyle = window.getComputedStyle(cell);
        var cellPaddingLeft = parseFloat(cellStyle.paddingLeft) || 0;
        var cellPaddingRight = parseFloat(cellStyle.paddingRight) || 0;
        var totalCellPadding = cellPaddingLeft + cellPaddingRight;
        
        var neededCellWidth = titleWidth + totalCellPadding;
        cell.style.width = neededCellWidth + 'px';
        cell.style.minWidth = neededCellWidth + 'px';
        cell.style.maxWidth = neededCellWidth + 'px';
        return;
      }
      
      // Measure all tag widths
      var tagWidths = [];
      allTags.forEach(function(tag) {
        tag.style.display = 'inline-flex';
        tag.style.position = 'absolute';
        tag.style.visibility = 'hidden';
        tagWidths.push({
          element: tag,
          width: tag.offsetWidth
        });
        tag.style.display = '';
        tag.style.position = '';
        tag.style.visibility = '';
      });
      
      // Measure button width
      var buttonWidth = 0;
      if (expandBtn) {
        expandBtn.style.display = 'inline-flex';
        expandBtn.style.position = 'absolute';
        expandBtn.style.visibility = 'hidden';
        buttonWidth = expandBtn.offsetWidth;
        expandBtn.style.display = '';
        expandBtn.style.position = '';
        expandBtn.style.visibility = '';
      }
      
      // Calculate row-based layout (max 3 rows)
      var rows = [[], [], []];
      var rowWidths = [0, 0, 0];
      var currentRow = 0;
      var visibleTagIndexes = [];
      
      for (var i = 0; i < tagWidths.length && currentRow < 3; i++) {
        var tagData = tagWidths[i];
        var needsWidth = tagData.width;
        var gapWidth = rowWidths[currentRow] > 0 ? gap : 0;
        
        if (rowWidths[currentRow] + gapWidth + needsWidth <= containerMaxWidth) {
          // Fits in current row
          rows[currentRow].push(tagData);
          rowWidths[currentRow] += gapWidth + needsWidth;
          visibleTagIndexes.push(i);
        } else if (currentRow < 2) {
          // Move to next row
          currentRow++;
          rows[currentRow].push(tagData);
          rowWidths[currentRow] = needsWidth;
          visibleTagIndexes.push(i);
        } else {
          // Already on row 3 and tag doesn't fit - stop
          break;
        }
      }
      
      var hiddenCount = tagWidths.length - visibleTagIndexes.length;
      
      // Reserve space for expand button in row 3 if needed
      if (hiddenCount > 0 && expandBtn && rows[2].length > 0) {
        var row3Width = rowWidths[2];
        var buttonNeeds = gap + buttonWidth;
        
        // Remove tags from row 3 until button fits
        while (row3Width + buttonNeeds > containerMaxWidth && rows[2].length > 0) {
          var removed = rows[2].pop();
          var removedIndex = visibleTagIndexes[visibleTagIndexes.length - 1];
          visibleTagIndexes.pop();
          
          // Recalculate row 3 width
          row3Width = 0;
          for (var j = 0; j < rows[2].length; j++) {
            row3Width += rows[2][j].width + (j > 0 ? gap : 0);
          }
          rowWidths[2] = row3Width;
          hiddenCount++;
        }
      }
      
      // Apply visibility based on visible indexes
      allTags.forEach(function(tag, idx) {
        if (visibleTagIndexes.indexOf(idx) >= 0) {
          tag.classList.remove('hidden-tag');
          tag.classList.add('visible-tag');
          tag.style.display = '';
        } else {
          tag.classList.add('hidden-tag');
          tag.classList.remove('visible-tag');
          tag.style.display = 'none';
        }
      });
      
      // Update button
      if (expandBtn) {
        if (hiddenCount > 0) {
          expandBtn.textContent = '+' + hiddenCount;
          expandBtn.style.display = '';
          expandBtn.dataset.originalText = '+' + hiddenCount;
        } else {
          expandBtn.style.display = 'none';
        }
      }
      
      // Calculate cell width: use the widest row (not containerMaxWidth unless needed)
      var maxRowWidth = Math.max(
        rowWidths[0] || 0,
        rowWidths[1] || 0,
        rowWidths[2] + (hiddenCount > 0 && expandBtn ? gap + buttonWidth : 0)
      );
      
      // Also check if any hidden tags are wider than the calculated width
      // (to prevent layout shift when expanding)
      var maxHiddenTagWidth = 0;
      for (var i = 0; i < tagWidths.length; i++) {
        if (visibleTagIndexes.indexOf(i) < 0) {
          // This is a hidden tag
          if (tagWidths[i].width > maxHiddenTagWidth) {
            maxHiddenTagWidth = tagWidths[i].width;
          }
        }
      }
      
      // Get cell padding
      var cellStyle = window.getComputedStyle(cell);
      var cellPaddingLeft = parseFloat(cellStyle.paddingLeft) || 0;
      var cellPaddingRight = parseFloat(cellStyle.paddingRight) || 0;
      var totalCellPadding = cellPaddingLeft + cellPaddingRight;
      
      // Cell width = max(widest visible row, widest hidden tag, title width) + padding
      var neededCellWidth = Math.max(maxRowWidth, maxHiddenTagWidth, titleWidth) + totalCellPadding;
      
      // Lock cell width
      cell.style.width = neededCellWidth + 'px';
      cell.style.minWidth = neededCellWidth + 'px';
      cell.style.maxWidth = neededCellWidth + 'px';
    });
    
    // Lock tbody cell widths to match header widths
    var headerCells = document.querySelectorAll('#musgu-table thead th');
    var bodyRows = document.querySelectorAll('#musgu-table tbody tr');
    
    // Calculate the widest Model column based on model names only (not affiliations)
    var maxModelNameWidth = 0;
    bodyRows.forEach(function(row) {
      var cell = row.cells[0];
      if (cell) {
        var modelNameElement = cell.querySelector('.model-name');
        if (modelNameElement) {
          // Measure just the model name element's natural width
          var tempSpan = document.createElement('span');
          tempSpan.style.visibility = 'hidden';
          tempSpan.style.position = 'absolute';
          tempSpan.style.whiteSpace = 'nowrap';
          tempSpan.style.fontWeight = 'bold';
          tempSpan.style.fontSize = '13px';
          tempSpan.textContent = modelNameElement.textContent;
          document.body.appendChild(tempSpan);
          var modelNameWidth = tempSpan.offsetWidth;
          document.body.removeChild(tempSpan);
          
          if (modelNameWidth > maxModelNameWidth) {
            maxModelNameWidth = modelNameWidth;
          }
        }
      }
    });
    
    // Set the Model column width based on model names only
    if (maxModelNameWidth > 0) {
      // Add cell padding (4px left + 4px right from td padding = 8px)
      var finalModelColumnWidth = maxModelNameWidth + 8;
      
      var modelHeaderCell = headerCells[0];
      if (modelHeaderCell) {
        modelHeaderCell.style.width = finalModelColumnWidth + 'px';
        modelHeaderCell.style.minWidth = finalModelColumnWidth + 'px';
        modelHeaderCell.style.maxWidth = finalModelColumnWidth + 'px';
      }
      
      bodyRows.forEach(function(row) {
        var cell = row.cells[0];
        if (cell) {
          cell.style.width = finalModelColumnWidth + 'px';
          cell.style.minWidth = finalModelColumnWidth + 'px';
          cell.style.maxWidth = finalModelColumnWidth + 'px';
        }
      });
    }
    
    // Show the table after calculations
    if (table) {
      table.classList.remove('calculating');
    }
  }
  
  expandButtons.forEach(function(btn) {
    // Store original text
    btn.dataset.originalText = btn.textContent;
    
    btn.addEventListener('click', function(e) {
      e.stopPropagation();
      var tagsContainer = this.closest('.criterion-tags');
      
      if (tagsContainer) {
        var isExpanded = tagsContainer.classList.contains('expanded');
        
        if (isExpanded) {
          // Collapsing - restore original text and hide tags again
          this.textContent = this.dataset.originalText;
          tagsContainer.classList.remove('expanded');
        } else {
          // Expanding - change button to minus and show all tags
          this.textContent = 'âˆ’';
          tagsContainer.classList.add('expanded');
        }
      }
    });
  });
}
</script>
</body>
</html>
